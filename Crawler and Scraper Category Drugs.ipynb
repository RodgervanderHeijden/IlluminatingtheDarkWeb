{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homemade Coocked Data Crawler and Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socks\n",
    "import socket\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Connect to TOR proxy\n",
    "socks.set_default_proxy(socks.SOCKS5, \"localhost\", 9050) \n",
    "socket.socket = socks.socksocket\n",
    "\n",
    "# Perform DNS resolution through the socket\n",
    "def getaddrinfo(*args):\n",
    "    return [(socket.AF_INET, socket.SOCK_STREAM, 6, '', (args[0], args[1]))]\n",
    "socket.getaddrinfo = getaddrinfo\n",
    "\n",
    "# URL of Torrez, category 'Drugs & Chemicals'\n",
    "url = \"http://yxuy5oau7nugw4kpb4lclrqdbixp3wvc4iuiad23ebyp2q3gx7rtrgqd.onion/items/category/drugs-and-chemicals?order=none&vendors=all&shipping_to=any&shipping_from=any&product_type=any&payments_method=any&page=1\"\n",
    "\n",
    "\n",
    "# Manually select and past the browser header after solving the captcha here\n",
    "cookies_copied = 'Cookie: ray_id=9bc0af157bce9cafa0a2b005a3ddc26d141c4aa7308e2e496c1cfbb089a07031; hr4ujvby8ds459og4kzcpbzwjdj_session=eyJpdiI6IjBIYVYzcWhrcng0dXFQazNaYlJxV2c9PSIsInZhbHVlIjoiWm9WOXJ6N3l0bUkxaXVyUGNcLzBpSXFrQ3VCMEFkNDRaYUplS2d6OVd5a09TOFdENm96UzFNdFoxdE9FYkFxa1wvTWVldkRqMktVT29JNWxIQUlaRmNpWnpPc0RWTXFUSEZJTnd6TFU5N2Nja1dVSXUxUXdTZTRVZ2V4RnJ2UW1pbSIsIm1hYyI6IjIzZjFlN2Y0MWE4MjRjMzIyN2UwZDQxMGRhNmFlOGJmODFjZmEwMDUzMTFjYTJlYTdjM2E3YmJiMjRlODM5NWUifQ%3D%3D; XSRF-TOKEN=eyJpdiI6ImxWelVNWUFiK3pJTE9iUitvRHRySUE9PSIsInZhbHVlIjoiaXhoMXloNVNFbU9pN3NrTWNQb0xKbkJibWtBMnNxdHFoYWJ4MkRhcmRWMjI2RUJkbWpwMGV0cFNLSGUxcDdVVENRbUdCSXBENm13SGlsUmRBTEJ0WloxaU1adnZlakxVdnVtSHo0WTR3cHV3N0Z1ZTBzZk02SFRxYlV5SkRBQ2QiLCJtYWMiOiJmMTVlMzMyZTkxNWZkNzJhOTNhNWQ3YzlhNjA1YzU2NmE1NmQzNDFlYzMzMDE3NmY3ZjkyMjdiYTc5YmUxNzlkIn0%3D'\n",
    "\n",
    "\n",
    "# Takes the relevant info of the copied cookies and puts it in the correct format automatically.\n",
    "def convert_pasted_cookies_to_usable_format(pasted_cookies):\n",
    "    all_cookies_str = pasted_cookies.split(':')[1] # to ignore the prefix \"Cookie: \")\n",
    "    individual_cookies = all_cookies_str.split(';')\n",
    "    cookies_set = {}\n",
    "    for cookie in individual_cookies:\n",
    "        cookies_set.update({cookie.strip().split('=')[0]: cookie.strip().split('=')[1]})\n",
    "    return cookies_set\n",
    "\n",
    "# request\n",
    "s = requests.Session()\n",
    "s.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; rv:78.0) Gecko/20100101 Firefox/78.0'})\n",
    "proxies = {\n",
    "    'http': 'socks5h://localhost:9050',\n",
    "    'https': 'socks5h://localhost:9050'}\n",
    "\n",
    "set_of_cookies = convert_pasted_cookies_to_usable_format(cookies_copied)\n",
    "\n",
    "# Get the page category drugs\n",
    "web_page = s.get(url, cookies=set_of_cookies, proxies=proxies)\n",
    "soup = BeautifulSoup(web_page.content, 'html.parser')\n",
    "\n",
    "\n",
    "# create empty lists of variables to scrape\n",
    "vendor = []    \n",
    "vendor_offers = []\n",
    "all_titles = []\n",
    "all_quantities = []\n",
    "all_prices = []\n",
    "category = []\n",
    "category_level_1 = []\n",
    "category_level_2 = []\n",
    "category_level_3 = []\n",
    "ship_from = []\n",
    "ship_to =[]\n",
    "\n",
    "\n",
    "# find the number of pages\n",
    "pages = []\n",
    "for a in soup.find_all('a',attrs={\"class\" : \"page-link\"}):\n",
    "    pages.append(a.text)\n",
    "last_page = pages[-2]\n",
    "\n",
    "# crawl through all pages in the category\n",
    "for page in range(1, int(last_page)+1):\n",
    "\n",
    "    # Base URL\n",
    "    url = 'http://yxuy5oau7nugw4kpb4lclrqdbixp3wvc4iuiad23ebyp2q3gx7rtrgqd.onion/items/category/drugs-and-chemicals?order=none&vendors=all&shipping_to=any&shipping_from=any&product_type=any&payments_method=any&page=' + str(page)\n",
    "\n",
    "    # connect and get content in HTML for scraping using BautifulSoup\n",
    "    web_page = s.get(url, cookies=set_of_cookies, proxies=proxies)\n",
    "    soup = BeautifulSoup(web_page.content, 'html.parser')\n",
    "    \n",
    "    # scrape the product names\n",
    "    for a in soup.find_all('a', href=True, class_='title'):\n",
    "        all_titles.append(a.text.strip())\n",
    "\n",
    "    # scrape the prices\n",
    "    for price in soup.find_all('span', class_='Price'):\n",
    "        all_prices.append(re.findall(\"\\d+\\.\\d+\", price.text.strip())[0])\n",
    "    \n",
    "    # scrape the category levels (3 levels + highest level)\n",
    "    for a in soup.find_all('div',attrs={\"class\" : \"w-100 mt-3 small\"}):\n",
    "        if len(a.find_all('a'))==4:\n",
    "            category_level_1.append(a.find_all('a')[1].text.strip())\n",
    "            category_level_2.append(a.find_all('a')[2].text.strip())\n",
    "            category_level_3.append(a.find_all('a')[3].text.strip())\n",
    "        elif len(a.find_all('a'))==3:\n",
    "            category_level_1.append(a.find_all('a')[1].text.strip())\n",
    "            category_level_2.append(a.find_all('a')[2].text.strip())\n",
    "            category_level_3.append(np.nan)\n",
    "        elif len(a.find_all('a'))==2:\n",
    "            category_level_1.append(a.find_all('a')[1].text.strip())\n",
    "            category_level_2.append(np.nan)\n",
    "            category_level_3.append(np.nan)\n",
    "        last_item = a.find_all('a')[-1]\n",
    "        category.append(last_item.text.strip())\n",
    "\n",
    "    # scrape the 'shipping from' locations       \n",
    "    for tag_elm in soup.find_all('div', class_='shipping'):\n",
    "        if tag_elm.find('span', class_='shippingFrom') != None:\n",
    "            ship_from.append(tag_elm.find('span', class_='shippingFrom').text.strip())\n",
    "        else:\n",
    "            ship_from.append(np.nan)\n",
    "            \n",
    "    # scrape the 'shipping to' locations\n",
    "    for tag_elm in soup.find_all('div', class_='shipping'):\n",
    "        if tag_elm.find('span', class_='shippingTo') != None:\n",
    "            ship_to.append(tag_elm.find('span', class_='shippingTo').text.strip())\n",
    "        else:\n",
    "            ship_to.append(np.nan)\n",
    "\n",
    "    # scrape the vendor names\n",
    "    for a in soup.find_all('a', class_='font-weight-bold'):\n",
    "        vendor.append(a.text.strip())\n",
    "\n",
    "\n",
    "            \n",
    "# combine all scraped information in tabular format            \n",
    "results = (list(zip(vendor, category_level_1, category_level_2, category_level_3, category, all_titles, \n",
    "                    all_prices, ship_from, ship_to)))\n",
    "\n",
    "df = pd.DataFrame(results,columns=['vendor', 'category_level_1', 'category_level_2', 'category_level_3', \n",
    "                                   'highest_category', 'product', 'price in $', 'shipping_from', 'shipping_to'])    \n",
    "\n",
    "# convert to csv\n",
    "df.to_csv('Product_dataset_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vendor information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique vendors\n",
    "unique_vendors = list(dict.fromkeys(vendor))\n",
    "\n",
    "# create empty list to store all vendors pages url in\n",
    "vendor_page = []\n",
    "\n",
    "# Get all the urls of the vendor pages\n",
    "for vendor in unique_vendors:\n",
    "    vendor_page.append(\"http://yxuy5oau7nugw4kpb4lclrqdbixp3wvc4iuiad23ebyp2q3gx7rtrgqd.onion/profile/\" + str(vendor))\n",
    "\n",
    "# create empty lists of variables to scrape\n",
    "since = [] \n",
    "transactions = []\n",
    "feedback_total = []\n",
    "feedback_positive = []\n",
    "feedback_negative = []\n",
    "disputes_total =[]\n",
    "disputes_won = []\n",
    "disputes_lost = []\n",
    "finalize_early = []\n",
    "rank = []\n",
    "verification = []\n",
    "\n",
    "# these variables were on the same section in the HTML code, so lets combine them for efficiency\n",
    "all_variables = [since, transactions, feedback_total, feedback_positive, \n",
    "                 feedback_negative, disputes_total, disputes_won, disputes_lost, finalize_early]\n",
    "all_text = ['On ToRReZ Market Since:', 'Total Amount Of Transactions', 'Total Feedback Received', \n",
    "            'Positive Feedback Received Ratio', 'Negative Feedback Received Ratio', 'Disputes Total:', \n",
    "            'Disputes Won:', 'Disputes Lost:', 'Finalize Early']    \n",
    "\n",
    "# crawl through all the pages of the vendors\n",
    "for page in vendor_page:\n",
    "    # connect and get content in HTML for scraping using BautifulSoup\n",
    "    web_page = s.get(page, cookies=set_of_cookies, proxies=proxies)\n",
    "    soup = BeautifulSoup(web_page.content, 'html.parser')\n",
    "    \n",
    "    # scrape all information\n",
    "    for numb in range(0,len(all_variables)):\n",
    "        for i in soup.find_all('table', class_='table table-borderless table-sm mt-1 table-fixed'):\n",
    "            value = soup.find('th', text=all_text[numb])\n",
    "            all_variables[numb].append(value.find_next_sibling('td').text.strip())\n",
    "\n",
    "    # scrape the rank\n",
    "    for span in soup.find_all('span', class_='badge badge-success question'):\n",
    "        rank.append(span.text.strip())\n",
    "    \n",
    "    # scrape the verification level\n",
    "    for tag_elm in soup.find_all('div', class_='col singleItemDetails mt-2'):\n",
    "        if tag_elm.find('a', class_='badge badge-dark') != None:\n",
    "            verification.append(tag_elm.find('a', class_='badge badge-dark').text.strip())\n",
    "        else:\n",
    "            verification.append(np.nan)\n",
    "            \n",
    "\n",
    "# combine all scraped information in tabular format         \n",
    "results_2 = (list(zip(unique_vendors, rank, verification, since, transactions, feedback_total, feedback_positive, feedback_negative, \n",
    "                      disputes_total, disputes_won, disputes_lost, finalize_early)))\n",
    "\n",
    "df_2 = pd.DataFrame(results_2,columns=['vendor', 'rank', 'verifcation', 'since', 'transactions', 'feedback_total', \n",
    "                                   'feedback_positive', 'feedback_negative','disputes_total',\n",
    "                                   'disputes_won', 'disputes_lost', 'finalize_early'])\n",
    "\n",
    "# convert to csv\n",
    "df_2.to_csv('Vendor_dataset_new.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
